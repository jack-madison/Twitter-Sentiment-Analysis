library(tidyverse)
library(readr)
# Import the 2017, 2018, and 2019 datasets
oseti_tweets_mun_2019 <- read_csv("oseti_tweets_mun_2019.csv")
oseti_tweets_mun_2018 <- read_csv("oseti_tweets_mun_2018.csv")
oseti_tweets_mun_2017 <- read_csv("oseti_tweets_mun_2017.csv")
# Merge the yearly datasets
tweets <- rbind(oseti_tweets_mun_2017, oseti_tweets_mun_2018, oseti_tweets_mun_2019)
# Group the tweets by mun_id to calculate the total number of tweets per municipality
tweets_per_mun <- tweets %>% group_by(mun_id) %>% summarise(tweet_count = sum(tweet_count))
# Extract the municipality info from the tweets dataframe
mun_info <- unique(tweets[c("mun_id", "municipality", "mun_X", "mun_Y")])
# Merge mun_info with tweets_per_mun to make the dataframe that we will map
tweets_per_mun <- full_join(mun_info, tweets_per_mun, by = "mun_id")
# Get the Japan shapefile at the municipality level and convert it to a dataframe
jpn2 <- getData("GADM", country = "JPN", level = 2)
jpn2_df <- tidy(jpn2, region = "NAME_2")
library(sp)
library(raster)
library(broom)
library(gpclib)
library(rgeos)
library(rgdal)
library(maptools)
library(tidyverse)
library(readr)
library(sp)
library(raster)
library(broom)
# Get the Japan shapefile at the municipality level and convert it to a dataframe
jpn2 <- getData("GADM", country = "JPN", level = 2)
plot(jpn2)
install.packages("geodata")
jpn2 <- gadm(country="JPN", level=2, path=tempdir())
library(geodata)
jpn2 <- gadm(country="JPN", level=2, path=tempdir())
plot(jpn2)
View(jpn2)
jpn2 <- getData("GADM", country = "JPN", level = 2)
plot(jpn2)
plot(jpn2)
jpn2 <- getData("GADM", country = "JPN", level = 2)
plot(jpn2)
View(oseti_tweets_mun_2017)
jpn2_df <- tidy(jpn2, region = "NAME_2")
id <- unique(jpn2_df$id)
id
View(jpn2_df)
install.packages("ggmap")
library(ggmap)
geocode("Abashiri")
install.packages("rstudioapi")
install.packages("rstudioapi")
library(rstudioapi)
register_google(key = "AIzaSyCRUpc6u2fURqkLDf--Hi1lGw4qbnJaOJ8")
library(rstudioapi)
register_google(key = "AIzaSyCRUpc6u2fURqkLDf--Hi1lGw4qbnJaOJ8")
library(ggmap)
register_google(key = "AIzaSyCRUpc6u2fURqkLDf--Hi1lGw4qbnJaOJ8")
geocode("Abashiri")
geocode("Abashiri, Japan")
geocode("Abiko, Japan")
register_google(key = "AIzaSyBYSCM6Y1kaV5m_Gb1YpO11pGQiEU0_fLU")
geocode("Abiko, Japan")
geocode("Abu, Japan")
city <- geocode("Abu, Japan")
View(city)
citys <- unique(jpn2_df$id)
geocoded <- data.frame()
citys <- data.frame(unique(jpn2_df$id))
library(tidyverse)
library(readr)
library(sp)
library(raster)
library(broom)
library(ggmap)
library(rstudioapi)
register_google(key = "AIzaSyBYSCM6Y1kaV5m_Gb1YpO11pGQiEU0_fLU")
# Import the 2017, 2018, and 2019 datasets
oseti_tweets_mun_2019 <- read_csv("oseti_tweets_mun_2019.csv")
oseti_tweets_mun_2018 <- read_csv("oseti_tweets_mun_2018.csv")
oseti_tweets_mun_2017 <- read_csv("oseti_tweets_mun_2017.csv")
# Merge the yearly datasets
tweets <- rbind(oseti_tweets_mun_2017, oseti_tweets_mun_2018, oseti_tweets_mun_2019)
# Group the tweets by mun_id to calculate the total number of tweets per municipality
tweets_per_mun <- tweets %>% group_by(mun_id) %>% summarise(tweet_count = sum(tweet_count))
# Extract the municipality info from the tweets dataframe
mun_info <- unique(tweets[c("mun_id", "municipality", "mun_X", "mun_Y")])
# Merge mun_info with tweets_per_mun to make the dataframe that we will map
tweets_per_mun <- full_join(mun_info, tweets_per_mun, by = "mun_id")
# Get the geodata from GADM
jpn2 <- getData("GADM", country = "JPN", level = 2)
jpn2_df <- tidy(jpn2, region = "NAME_2")
city_df <- data.frame(unique(jpn2_df$id))
View(city_df)
colnames <- c("city")
colnames(city_df) <- c("city")
View(city_df)
geocode("Abu, Japan")
for (i in 1:nrow(city_df)) {
result <- geocode(city_df$city[i], output = "latlona")
city_df$lon[i] <- as.numeric(result[1])
city_df$lat[i] <- as.numeric(result[2])
}
for (i in 1:10) {
result <- geocode(city_df$city[i])
city_df$lon[i] <- as.numeric(result[1])
city_df$lat[i] <- as.numeric(result[2])
}
library(tidyverse)
library(readr)
library(sp)
library(raster)
library(broom)
library(ggmap)
library(rstudioapi)
register_google(key = "AIzaSyBYSCM6Y1kaV5m_Gb1YpO11pGQiEU0_fLU")
# Import the 2017, 2018, and 2019 datasets
oseti_tweets_mun_2019 <- read_csv("oseti_tweets_mun_2019.csv")
oseti_tweets_mun_2018 <- read_csv("oseti_tweets_mun_2018.csv")
oseti_tweets_mun_2017 <- read_csv("oseti_tweets_mun_2017.csv")
# Merge the yearly datasets
tweets <- rbind(oseti_tweets_mun_2017, oseti_tweets_mun_2018, oseti_tweets_mun_2019)
# Group the tweets by mun_id to calculate the total number of tweets per municipality
tweets_per_mun <- tweets %>% group_by(mun_id) %>% summarise(tweet_count = sum(tweet_count))
# Extract the municipality info from the tweets dataframe
mun_info <- unique(tweets[c("mun_id", "municipality", "mun_X", "mun_Y")])
# Merge mun_info with tweets_per_mun to make the dataframe that we will map
tweets_per_mun <- full_join(mun_info, tweets_per_mun, by = "mun_id")
# Get the geodata from GADM
jpn2 <- getData("GADM", country = "JPN", level = 2)
jpn2_df <- tidy(jpn2, region = "NAME_2")
city_df <- data.frame(unique(jpn2_df$id))
colnames(city_df) <- c("city")
for (i in 1:10) {
result <- geocode(city_df$city[i])
city_df$lon[i] <- as.numeric(result[1])
city_df$lat[i] <- as.numeric(result[2])
}
View(city_df)
city_df <- data.frame(unique(jpn2_df$id))
colnames(city_df) <- c("city")
for (i in 1:nrow(city_df)) {
result <- geocode(city_df$city[i])
city_df$lon[i] <- as.numeric(result[1])
city_df$lat[i] <- as.numeric(result[2])
}
library(tidyverse)
library(readr)
library(sp)
library(raster)
library(broom)
library(ggmap)
library(rstudioapi)
register_google(key = "AIzaSyBYSCM6Y1kaV5m_Gb1YpO11pGQiEU0_fLU")
# Import the 2017, 2018, and 2019 datasets
oseti_tweets_mun_2019 <- read_csv("oseti_tweets_mun_2019.csv")
oseti_tweets_mun_2018 <- read_csv("oseti_tweets_mun_2018.csv")
oseti_tweets_mun_2017 <- read_csv("oseti_tweets_mun_2017.csv")
# Merge the yearly datasets
tweets <- rbind(oseti_tweets_mun_2017, oseti_tweets_mun_2018, oseti_tweets_mun_2019)
# Group the tweets by mun_id to calculate the total number of tweets per municipality
tweets_per_mun <- tweets %>% group_by(mun_id) %>% summarise(tweet_count = sum(tweet_count))
# Extract the municipality info from the tweets dataframe
mun_info <- unique(tweets[c("mun_id", "municipality", "mun_X", "mun_Y")])
# Merge mun_info with tweets_per_mun to make the dataframe that we will map
tweets_per_mun <- full_join(mun_info, tweets_per_mun, by = "mun_id")
# Get the geodata from GADM
jpn2 <- getData("GADM", country = "JPN", level = 2)
jpn2_df <- tidy(jpn2, region = "NAME_2")
city_df <- data.frame(unique(jpn2_df$id))
colnames(city_df) <- c("city")
city_df$city_japan <- city_df$city + ", Japan"
View(city_df)
city_df$city_japan <- paste(city_df$city, ", Japan")
View(city_df)
for (i in 1:nrow(city_df)) {
result <- geocode(city_df$city_japan[i])
city_df$lon[i] <- as.numeric(result[1])
city_df$lat[i] <- as.numeric(result[2])
}
View(city_df)
city_df_missing <- city_df[is.na(city_df$lon), ]
View(city_df_missing)
View(city_df)
# Set Asakura lat manually
city_df$lon[65] <- as.numeric(130.66565696500018)
city_df$lat[65] <- as.numeric(33.423355345642065)
# Set Asakura lat manually
city_df$lon[65] <- as.numeric(130.66565696500018)
city_df$lat[65] <- as.numeric(33.423355345642065)
# Set Fuji Manually
city_df$lon[154] <- as.numeric(138.67624139992301)
city_df$lat[154] <- as.numeric(35.1615654218984)
# Set Midori Manually
city_df$lon[765] <- as.numeric(139.2732842846065)
city_df$lat[765] <- as.numeric(36.43298967377905)
# Set Sakura Manually
city_df$lon[1170] <- as.numeric(140.2240090321607)
city_df$lat[1170] <- as.numeric(35.72362777755255)
rm(oseti_tweets_mun_2017)
rm(c(oseti_tweets_mun_2017, oseti_tweets_mun_2018, oseti_tweets_mun_2019))
rm(c(oseti_tweets_mun_2018, oseti_tweets_mun_2019))
rm(oseti_tweets_mun_2018)
rm(oseti_tweets_mun_2019)
rm(jpn2)
rm(city_df_missing)
rm(result)
rm(i)
library(geosphere)
View(city_df)
View(tweets_per_mun)
View(city_df)
# I then find the closest city to each city in tweets_per_mun from city_df
mat <- distm(city_df[,c('lon','lat')], tweets_per_mun[,c('mun_X','mun_Y')], fun=distVincentyEllipsoid)
tweets_per_mun$city <- city_df$city[apply(mat, 1, which.min)]
View(mun_info)
View(tweets)
View(tweets_per_mun)
install.packages("rgeos")
library(rgeos)
tweets_locations <- SpatialPoints(tweets_per_mun)
tweets_locations <- SpatialPoints(tweets_per_mun["lat", "lon"])
tweets_latlon <- tweets_per_mun[c("lon", "lat")]
tweets_per_mun[c("lon", "lat")]
View(tweets_per_mun)
set2sp <- SpatialPoints(city_df)
tweets_latlon <- tweets_per_mun[c("mun_X", "mun_Y")]
set1sp <- SpatialPoints(tweets_latlon)
tweets_per_mun$city <- city_df$city[apply(mat, 1, which.min)]
library(tidyverse)
library(readr)
library(sp)
library(raster)
library(broom)
library(ggmap)
library(rstudioapi)
library(geosphere)
library(rgeos)
register_google(key = "AIzaSyBYSCM6Y1kaV5m_Gb1YpO11pGQiEU0_fLU")
# Get the geodata from GADM
jpn2 <- getData("GADM", country = "JPN", level = 2)
jpn2_df <- tidy(jpn2, region = "NAME_2")
rm(jpn2)
# Create a dataframe with the names of each of the cities from jpn2_df
city_df <- data.frame(unique(jpn2_df$id))
colnames(city_df) <- c("city")
city_df$city_japan <- paste(city_df$city, ", Japan")
# For each of the cities get the lat and lon from the google maps API
for (i in 1:nrow(city_df)) {
result <- geocode(city_df$city_japan[i])
city_df$lon[i] <- as.numeric(result[1])
city_df$lat[i] <- as.numeric(result[2])
}
# The google maps API was unable to find the lat and lon for 4 cities so:
# Set Asakura lat manually
city_df$lon[65] <- as.numeric(130.66565696500018)
city_df$lat[65] <- as.numeric(33.423355345642065)
# Set Fuji Manually
city_df$lon[154] <- as.numeric(138.67624139992301)
city_df$lat[154] <- as.numeric(35.1615654218984)
# Set Midori Manually
city_df$lon[765] <- as.numeric(139.2732842846065)
city_df$lat[765] <- as.numeric(36.43298967377905)
# Set Sakura Manually
city_df$lon[1170] <- as.numeric(140.2240090321607)
city_df$lat[1170] <- as.numeric(35.72362777755255)
View(city_df)
View(jpn2_df)
View(city_df)
city_df <- subset(city_df, select = -c("city_japan"))
View(city_df)
city_df <- subset(city_df, select = c("city", "lon", "lat"))
View(city_df)
write.csv(city_df, "map_cities.csv", row.names = FALSE)
library(tidyverse)
library(readr)
library(sp)
library(raster)
library(broom)
library(gpclib)
library(rgeos)
library(rgdal)
library(maptools)
oseti_tweets_mun_2019 <- read_csv("oseti_tweets_mun_2019.csv")
oseti_tweets_mun_2018 <- read_csv("oseti_tweets_mun_2018.csv")
oseti_tweets_mun_2017 <- read_csv("oseti_tweets_mun_2017.csv")
tweets <- rbind(oseti_tweets_mun_2019, oseti_tweets_mun_2018, oseti_tweets_mun_2017)
rm(oseti_tweets_mun_2019)
rm(oseti_tweets_mun_2018)
rm(oseti_tweets_mun_2017)
cities <- read_csv("matched_cities.csv")
tweets <- merge(x = tweets, y = cities, by = "mun_id", all.x = TRUE)
tweet_counts <- tweets %>% group_by(city) %>% summarise(tweet_count = sum(tweet_count))
tweet_counts <- rename(tweet_counts, id = city)
jpn2 <- getData("GADM", country = "JPN", level = 2)
jpn2_df <- tidy(jpn2, region = "NAME_2")
df <- merge(x = jpn2_df, y = tweet_counts, by = 'id', all.x = TRUE)
View(jpn2_df)
ggplot() + geom_polygon(data = jpn2_df, aes(x = long, y = lat, group = group, fill = id))
library(scales)
ggplot() + geom_polygon(data = jpn2_df, aes(x = long, y = lat, group = group, fill = id))
library(tidyverse)
library(readr)
library(sp)
library(raster)
library(broom)
library(gpclib)
library(rgeos)
library(rgdal)
library(maptools)
oseti_tweets_mun_2019 <- read_csv("oseti_tweets_mun_2019.csv")
oseti_tweets_mun_2018 <- read_csv("oseti_tweets_mun_2018.csv")
oseti_tweets_mun_2017 <- read_csv("oseti_tweets_mun_2017.csv")
tweets <- rbind(oseti_tweets_mun_2019, oseti_tweets_mun_2018, oseti_tweets_mun_2017)
rm(oseti_tweets_mun_2019)
rm(oseti_tweets_mun_2018)
rm(oseti_tweets_mun_2017)
cities <- read_csv("matched_cities.csv")
tweets <- merge(x = tweets, y = cities, by = "mun_id", all.x = TRUE)
tweet_counts <- tweets %>% group_by(city) %>% summarise(tweet_count = sum(tweet_count))
tweet_counts <- rename(tweet_counts, id = city)
jpn2 <- getData("GADM", country = "JPN", level = 2)
jpn2_df <- tidy(jpn2, region = "NAME_2")
df <- merge(x = jpn2_df, y = tweet_counts, by = 'id', all.x = TRUE)
library(scales)
ggplot() + geom_polygon(data = jpn2_df, aes(x = long, y = lat, group = group, fill = id)) + theme_void() + theme(legend.position = "none")
ggplot() + geom_polygon(data = df, aes(x = long, y = lat, group = group, fill = tweet_count)) + theme_void()
df[is.na(df)] <- 0
ggplot() + geom_polygon(data = df, aes(x = long, y = lat, group = group, fill = tweet_count)) + theme_void()
ggplot() + geom_polygon(data = df, aes(x = long, y = lat, group = group, fill = id)) + theme_void() + theme(legend.position = "none")
View(df)
df <- merge(jpn2_df, tweet_counts, by = 'id')
View(df)
oseti_tweets_mun_2019 <- read_csv("oseti_tweets_mun_2019.csv")
oseti_tweets_mun_2018 <- read_csv("oseti_tweets_mun_2018.csv")
oseti_tweets_mun_2017 <- read_csv("oseti_tweets_mun_2017.csv")
tweets <- rbind(oseti_tweets_mun_2019, oseti_tweets_mun_2018, oseti_tweets_mun_2017)
rm(oseti_tweets_mun_2019)
rm(oseti_tweets_mun_2018)
rm(oseti_tweets_mun_2017)
cities <- read_csv("matched_cities.csv")
tweets <- merge(x = tweets, y = cities, by = "mun_id", all.x = TRUE)
tweet_counts <- tweets %>% group_by(city) %>% summarise(tweet_count = sum(tweet_count))
tweet_counts <- rename(tweet_counts, id = city)
View(tweet_counts)
jpn2 <- getData("GADM", country = "JPN", level = 2)
jpn2_df <- tidy(jpn2, region = "NAME_2")
ggplot() + geom_polygon(data = jpn2_df, aes(x = long, y = lat, group = group, fill = id)) + theme_void() + theme(legend.position = "none")
df <- merge(jpn2_df, tweet_counts, by = 'id')
View(df)
View(jpn2_df)
View(df)
id <- unique(jpn2_df$id)
df_city <- data.frame(id)
View(df_city)
View(cities)
library(ggplot2)
library(sp)
library(raster)
library(broom)
library(tidyverse)
library(gpclib)
library(rgeos)
library(readr)
library(rgdal)
library(maptools)
if (!require(gpclib)) install.packages("gpclib", type="source")
gpclibPermit()
jpn2 <- getData("GADM", country = "JPN", level = 2)
jpn2_df <- tidy(jpn2, region = "NAME_2")
jpn2_df$id <- str_replace_all(jpn2_df$id, "ō", "o")
jpn2_df$id <- str_replace_all(jpn2_df$id, "Ō", "O")
jpn2_df$id <- str_replace_all(jpn2_df$id, "ū", "u")
id <- jpn2_df %>% group_by(id) %>% summarise(long = mean(long), lat = mean(lat))
oseti_tweets <- read_csv("GitHub/Twitter-Sentiment-Analysis/4_tweets_aggregated/oseti_tweets.csv")
library(readr)
oseti_tweets_mun_2017 <- read_csv("oseti_tweets_mun_2017.csv")
View(oseti_tweets_mun_2017)
oseti_tweets_mun_2018 <- read_csv("oseti_tweets_mun_2018.csv")
oseti_tweets_mun_2019 <- read_csv("oseti_tweets_mun_2019.csv")
oseti_tweets <- rbind(oseti_tweets_mun_2017, oseti_tweets_mun_2018, oseti_tweets_mun_2019)
tweet_counts <- oseti_tweets %>% group_by(mun_id) %>% summarise(tweet_count = sum(tweet_count), long = mean(mun_X), lat = mean(mun_Y))
library(geosphere)
mat <- distm(id[,c('long','lat')], tweet_counts[,c('long','lat')], fun=distVincentyEllipsoid)
id$tweets <- tweet_counts$tweet_count[apply(mat, 1, which.min)]
id <- subset(id, select = c(id, tweets))
df <- merge(jpn2_df, id, by = 'id')
df$ln_tweets <- log(df$tweets)
my_breaks = c(50, 500, 5000, 50000, 500000)
library(scales)
ggplot() + geom_polygon(data = df, aes(x = long, y = lat, group = group, fill = tweets)) + theme_void() + coord_equal(xlim = c(127, 147), ylim = c(30,46)) + scale_fill_gradient(high = "#132B43", low = "#56B1F7", name = "Tweets in 2019", trans = "log", labels = comma, breaks = my_breaks) + ggtitle("Geographic Distribution of Tweets in Japan (2019)")
ggplot() + geom_polygon(data = df, aes(x = long, y = lat, group = group, fill = tweets)) + theme_void() + coord_equal(xlim = c(127, 147), ylim = c(30,46)) + scale_fill_gradient(high = "#132B43", low = "#56B1F7", name = "Tweets in 2019", trans = "log", labels = comma, breaks = my_breaks) + ggtitle("Geographic Distribution of Tweets in Japan (2017 to 2019)")
View(df)
View(df)
my_breaks = c(50, 500, 5000, 50000, 500000, 5000000)
library(scales)
ggplot() + geom_polygon(data = df, aes(x = long, y = lat, group = group, fill = tweets)) + theme_void() + coord_equal(xlim = c(127, 147), ylim = c(30,46)) + scale_fill_gradient(high = "#132B43", low = "#56B1F7", name = "Tweets in 2019", trans = "log", labels = comma, breaks = my_breaks) + ggtitle("Geographic Distribution of Tweets in Japan (2017 to 2019)")
my_breaks = c(10, 100, 1000, 10000, 100000, 1000000)
library(scales)
ggplot() + geom_polygon(data = df, aes(x = long, y = lat, group = group, fill = tweets)) + theme_void() + coord_equal(xlim = c(127, 147), ylim = c(30,46)) + scale_fill_gradient(high = "#132B43", low = "#56B1F7", name = "Tweets in 2019", trans = "log", labels = comma, breaks = my_breaks) + ggtitle("Geographic Distribution of Tweets in Japan (2017 to 2019)")
ggplot() + geom_polygon(data = df, aes(x = long, y = lat, group = group, fill = tweets)) + theme_void() + coord_equal(xlim = c(127, 147), ylim = c(30,46)) + scale_fill_gradient(high = "#132B43", low = "#56B1F7", name = "Total Tweets", trans = "log", labels = comma, breaks = my_breaks) + ggtitle("Geographic Distribution of Tweets in Japan (2017 to 2019)")
ggplot() + geom_polygon(data = df, aes(x = long, y = lat, group = group, fill = tweets)) + theme_void() + coord_equal(xlim = c(127, 147), ylim = c(30,46)) + scale_fill_gradient(high = "#132B43", low = "#56B1F7", name = "Total Tweets", trans = "log", labels = comma) + ggtitle("Geographic Distribution of Tweets in Japan (2017 to 2019)")
install.packages("viridis")
library(viridis)
ggplot() + geom_polygon(data = df, aes(x = long, y = lat, group = group, fill = tweets)) + theme_void() + coord_equal(xlim = c(127, 147), ylim = c(30,46)) + scale_fill_viridis(ame = "Total Tweets", trans = "log", labels = comma) + ggtitle("Geographic Distribution of Tweets in Japan (2017 to 2019)")
library(viridis)
ggplot() + geom_polygon(data = df, aes(x = long, y = lat, group = group, fill = tweets)) + theme_void() + coord_equal(xlim = c(127, 147), ylim = c(30,46)) + scale_fill_viridis(name = "Total Tweets", trans = "log", labels = comma) + ggtitle("Geographic Distribution of Tweets in Japan (2017 to 2019)")
my_breaks = c(100, 10000, 1000000)
ggplot() + geom_polygon(data = df, aes(x = long, y = lat, group = group, fill = tweets)) + theme_void() + coord_equal(xlim = c(127, 147), ylim = c(30,46)) + scale_fill_viridis(name = "Total Tweets", trans = "log", labels = comma, breaks = my_breaks ) + ggtitle("Geographic Distribution of Tweets in Japan (2017 to 2019)")
library(viridis)
ggplot() + geom_polygon(data = df, aes(x = long, y = lat, group = group, fill = tweets)) + theme_void() + theme(legend.position = c(140, 34)) + coord_equal(xlim = c(127, 147), ylim = c(30,46)) + scale_fill_viridis(name = "Total Tweets", trans = "log", labels = comma, breaks = my_breaks ) + ggtitle("Geographic Distribution of Tweets in Japan (2017 to 2019)")
ggplot() + geom_polygon(data = df, aes(x = long, y = lat, group = group, fill = tweets)) + theme_void() + theme(legend.position = c(.2, .85)) + coord_equal(xlim = c(127, 147), ylim = c(30,46)) + scale_fill_viridis(name = "Total Tweets", trans = "log", labels = comma, breaks = my_breaks ) + ggtitle("Geographic Distribution of Tweets in Japan (2017 to 2019)")
ggplot() + geom_polygon(data = df, aes(x = long, y = lat, group = group, fill = tweets)) + theme_void() + theme(legend.position = c(.8, .2)) + coord_equal(xlim = c(127, 147), ylim = c(30,46)) + scale_fill_viridis(name = "Total Tweets", trans = "log", labels = comma, breaks = my_breaks ) + ggtitle("Geographic Distribution of Tweets in Japan (2017 to 2019)")
ggplot() + geom_polygon(data = df, aes(x = long, y = lat, group = group, fill = tweets)) + theme_void() + theme(legend.position = c(.85, .35)) + coord_equal(xlim = c(127, 147), ylim = c(30,46)) + scale_fill_viridis(name = "Total Tweets", trans = "log", labels = comma, breaks = my_breaks ) + ggtitle("Geographic Distribution of Tweets in Japan (2017 to 2019)")
ggplot() + geom_polygon(data = df, aes(x = long, y = lat, group = group, fill = tweets)) + theme_void() + theme(legend.position = c(.85, .35), plot.title = element_text(hjust = 0.2, vjust = -0.2)) + coord_equal(xlim = c(127, 147), ylim = c(30,46)) + scale_fill_viridis(name = "Total Tweets", trans = "log", labels = comma, breaks = my_breaks ) + ggtitle("Geographic Distribution of Tweets in Japan (2017 to 2019)")
ggplot() + geom_polygon(data = df, aes(x = long, y = lat, group = group, fill = tweets)) + theme_void() + theme(legend.position = c(.85, .35), plot.title = element_text(hjust = 0.2, vjust = -10)) + coord_equal(xlim = c(127, 147), ylim = c(30,46)) + scale_fill_viridis(name = "Total Tweets", trans = "log", labels = comma, breaks = my_breaks ) + ggtitle("Geographic Distribution of Tweets in Japan (2017 to 2019)")
ggplot() + geom_polygon(data = df, aes(x = long, y = lat, group = group, fill = tweets)) + theme_void() + theme(legend.position = c(.85, .35), plot.title = element_text(hjust = 0.2, vjust = -30)) + coord_equal(xlim = c(127, 147), ylim = c(30,46)) + scale_fill_viridis(name = "Total Tweets", trans = "log", labels = comma, breaks = my_breaks ) + ggtitle("Geographic Distribution of Tweets in Japan (2017 to 2019)")
